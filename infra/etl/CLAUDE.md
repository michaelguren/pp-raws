# ETL Data Pipeline Architecture

**Read `../CLAUDE.md` first** for general infrastructure requirements.

## Principles
- **Convention over Configuration**: Predictable naming and structure
- **Factory Pattern**: Bronze layers use shared code via `EtlStackFactory`
- **Kill-and-Fill**: Complete data refresh each run
- **Config-Driven**: `config.json` files define all dataset behavior

## Architecture
```
EventBridge ‚Üí Glue Job ‚Üí S3 (raw/bronze/gold) ‚Üí Crawler ‚Üí Athena
```

## Directory Structure
```
etl/
‚îú‚îÄ‚îÄ config.json                        # Global config (prefixes, worker sizes)
‚îú‚îÄ‚îÄ index.js                           # Stack orchestrator
‚îú‚îÄ‚îÄ EtlCoreStack.js                   # Shared infrastructure (S3, IAM, databases)
‚îú‚îÄ‚îÄ datasets/                          # Organized by medallion layer (RAWS convention)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/{dataset}/             # Raw data ingestion datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json               # Dataset config (source URL, schema, schedule)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {Dataset}Stack.js         # Stack definition
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ glue/*.py                 # Custom bronze jobs (Pattern B only)
‚îÇ   ‚îú‚îÄ‚îÄ silver/{dataset}/             # Transformed/joined datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json               # Dataset config (dependencies, transformations)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ {Dataset}Stack.js         # Stack definition (always custom)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ glue/*.py                 # Custom transformation jobs
‚îÇ   ‚îî‚îÄ‚îÄ gold/{dataset}/               # Curated, analytics-ready datasets
‚îÇ       ‚îú‚îÄ‚îÄ config.json               # Dataset config (dependencies, temporal config)
‚îÇ       ‚îú‚îÄ‚îÄ {Dataset}Stack.js         # Stack definition (always custom)
‚îÇ       ‚îî‚îÄ‚îÄ glue/*.py                 # Custom gold layer jobs
‚îî‚îÄ‚îÄ shared/
    ‚îú‚îÄ‚îÄ deploytime/
    ‚îÇ   ‚îú‚îÄ‚îÄ factory.js                # CDK factory for bronze stacks
    ‚îÇ   ‚îî‚îÄ‚îÄ index.js                  # Shared deployment utilities
    ‚îî‚îÄ‚îÄ runtime/
        ‚îú‚îÄ‚îÄ https_zip/
        ‚îÇ   ‚îú‚îÄ‚îÄ bronze_http_job.py    # Shared bronze job for HTTP/ZIP sources
        ‚îÇ   ‚îî‚îÄ‚îÄ etl_runtime_utils.py  # Runtime utilities (packaged via --extra-py-files)
        ‚îî‚îÄ‚îÄ temporal/
            ‚îî‚îÄ‚îÄ temporal_versioning.py # Shared temporal versioning for gold layer
```

## Configuration Files

**Global** (`etl/config.json`): Resource naming, database prefixes, worker sizes

**Dataset** (`datasets/{dataset}/config.json`):
```json
{
  "dataset": "dataset-name",
  "source_url": "https://...",
  "data_size_category": "small|medium|large|xlarge",
  "delimiter": ",|\t||",
  "file_table_mapping": {
    "source.csv": "table-name"    // Hyphens ‚Üí underscores in Glue tables
  },
  "column_schema": {
    "table-name": {
      "Source Column": {
        "target_name": "snake_case",
        "type": "string|date|integer|long|float|double|decimal|boolean",
        "format": "yyyyMMdd",       // dates only
        "precision": "10,2"         // decimal only (optional, defaults to 10,2)
      }
    }
  }
}
```

## Naming Conventions

**AWS Resources**: `{prefix}-{layer}-{dataset}[-{table}][-crawler]`
- Glue Jobs: `pp-dw-bronze-fda-nsde`, `pp-dw-silver-fda-all-ndc`, `pp-dw-gold-fda-all-ndcs`
- Crawlers: `pp-dw-bronze-fda-products-crawler`, `pp-dw-gold-fda-all-ndcs-crawler`
- Databases: `pp_dw_bronze`, `pp_dw_silver`, `pp_dw_gold` (underscores, not hyphens)
- Glue Tables: Hyphens in dataset names ‚Üí underscores (`fda-all-ndcs` ‚Üí `fda_all_ndcs`)
- **Naming Convention**: Gold datasets use **plural names** for consistency

**S3 Paths**:
```
s3://pp-dw-{account}/
‚îú‚îÄ‚îÄ raw/{dataset}/run_id={timestamp}/
‚îú‚îÄ‚îÄ bronze/{dataset}/              # Single-table datasets
‚îú‚îÄ‚îÄ bronze/{dataset}/{table}/      # Multi-table datasets
‚îú‚îÄ‚îÄ silver/{dataset}/              # Silver layer tables
‚îú‚îÄ‚îÄ gold/{dataset}/                # Gold layer tables (status-partitioned)
‚îî‚îÄ‚îÄ etl/{dataset}/glue/            # Dataset-specific scripts
```

## Stack Types

### Bronze Layer Patterns

**Pattern A: Factory-Based (Standard HTTP/ZIP CSV Sources)**
Use for datasets with CSV/TSV files, standard delimiters, and column headers:
```javascript
const EtlStackFactory = require("../../shared/deploytime/factory");
const datasetConfig = require("./config.json");
const factory = new EtlStackFactory(this, props);

factory.createDatasetInfrastructure({
  datasetConfig,
  options: { skipSilverJob: true }  // Bronze only
});
```
- Uses shared `bronze_http_job.py` and `etl_runtime_utils.py`
- Schema defined in `config.json` (column mappings, types)
- Supports single and multi-table datasets
- Examples: `fda-nsde`, `fda-cder`

**Pattern B: Custom Bronze Jobs (Specialized Sources)**
Use for datasets with unique formats, authentication, or processing requirements:
- Custom `{Dataset}Stack.js` with manual Glue job creation
- Custom `glue/bronze_job.py` with specialized logic
- May have inline schema definitions or custom processing
- Examples:
  - **RxNORM**: RRF files (no headers, pipe-delimited, UMLS auth)
  - **APIs**: REST/GraphQL sources requiring pagination, auth tokens
  - **Binary formats**: Parquet, Avro, or proprietary formats

**Silver Stacks** (always custom, no factory):
- Read source dataset configs at deploy time for table names
- Pass table names as Glue job arguments
- Custom transformation logic in `glue/*.py`
- Examples:
  - `fda-all-ndc` joins `fda-nsde` + `fda-cder` tables
  - `rxnorm-ndc-mappings` extracts NDC mappings from RXNSAT
  - `rxnorm-products` filters prescribable products from RxNORM

## Gold Layer - Temporal Versioning (SCD Type 2)

Gold datasets use **temporal versioning** to enable efficient incremental DynamoDB syncs and historical tracking.

### Core Pattern
- **End-of-Time**: Current records use `active_to = '9999-12-31'` (not NULL)
- **Status Partitioning**: Physical separation into `status=current/` and `status=historical/`
- **Change Detection**: MD5 hash of business columns identifies changes
  - NEW: Insert with `active_to=9999-12-31`
  - CHANGED: Expire old (set `active_to=today`), insert new
  - UNCHANGED: Skip (no write)
  - EXPIRED: Set `active_to=today`

### Schema
```
# Version tracking
version_id       STRING    UUID for this version
content_hash     STRING    MD5 of business columns
active_from      DATE      When version became active
active_to        DATE      When expired (9999-12-31 = current)
status           STRING    Partition: current or historical

# Business columns (no layer prefixes)
ndc_11, proprietary_name, ...           (fda-all-ndcs)
rxcui, str, ingredient_names, ...       (rxnorm-products)
rxcui, ndc_11, ...                      (rxnorm-ndc-mappings - composite key)
rxcui, class_id, class_type, ...        (rxnorm-product-classifications - composite key)

# Metadata
run_date, run_id, source_file
```

### Shared Library
**Location**: `shared/runtime/temporal/temporal_versioning.py`

**Key Functions**:
```python
# Apply temporal versioning to incoming data
versioned_df = apply_temporal_versioning(
    spark, incoming_df, existing_table,
    business_key, business_columns, run_date, run_id
)

# Query helpers
get_current_records(spark, table)              # WHERE active_to = '9999-12-31'
get_changes_for_date(spark, table, date)       # Incremental sync query
```

### Common Queries
```sql
-- Current records only
SELECT * FROM pp_dw_gold.fda_all_ndcs WHERE status = 'current';

-- Incremental changes for DynamoDB sync
SELECT * FROM pp_dw_gold.fda_all_ndcs
WHERE run_date = '2025-10-14'
  AND (active_from = '2025-10-14' OR active_to = '2025-10-14');

-- Point-in-time historical query
SELECT * FROM pp_dw_gold.fda_all_ndcs
WHERE ndc_11 = '00002322702'
  AND '2024-01-01' BETWEEN active_from AND active_to;

-- Composite key query (mappings)
SELECT * FROM pp_dw_gold.rxnorm_ndc_mappings
WHERE rxcui = '392409' AND status = 'current';

-- Composite key query (classifications)
SELECT * FROM pp_dw_gold.rxnorm_product_classifications
WHERE rxcui = '392409' AND class_type = 'ATC' AND status = 'current';
```

### Benefits
- **95% cost reduction** in DynamoDB writes (~380K ‚Üí ~500 writes/day)
- **Historical queries** - view data as it existed at any point in time
- **Audit trail** - complete change history
- **Fast queries** - status partitioning enables partition pruning

---

## Reading from Glue Data Catalog

**CRITICAL: Always use `glueContext.create_dynamic_frame.from_catalog()` to read tables from the Glue Data Catalog.**

### Correct Pattern
```python
# ‚úÖ CORRECT - Use GlueContext to read from catalog
from awsglue.context import GlueContext
from pyspark.context import SparkContext

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Read table from Glue catalog and convert to DataFrame
rxnsat_df = glueContext.create_dynamic_frame.from_catalog(
    database="pp_dw_bronze",
    table_name="rxnsat"
).toDF()
```

### Incorrect Pattern
```python
# ‚ùå INCORRECT - spark.table() will fail with "TABLE_OR_VIEW_NOT_FOUND"
rxnsat_df = spark.table("pp_dw_bronze.rxnsat")  # DON'T DO THIS!

# ‚ùå INCORRECT - Even with catalog specified
rxnsat_df = spark.table(f"{database}.{table}")  # DON'T DO THIS!
```

### Why This Matters
- `spark.table()` requires proper Hive metastore configuration and catalog settings
- `glueContext.create_dynamic_frame.from_catalog()` is the AWS-native method that works reliably with Glue Data Catalog
- The GlueContext method handles all catalog configuration automatically
- This is the recommended pattern in AWS Glue documentation

### Common Usage Patterns

**Single table read:**
```python
df = glueContext.create_dynamic_frame.from_catalog(
    database=args['bronze_database'],
    table_name="rxnsat"
).toDF()
```

**Multiple table reads with job arguments:**
```python
# Stack passes table names as job arguments
rxnconso = glueContext.create_dynamic_frame.from_catalog(
    database=args['bronze_database'],
    table_name=args['rxnconso_table']
).toDF()

rxnrel = glueContext.create_dynamic_frame.from_catalog(
    database=args['bronze_database'],
    table_name=args['rxnrel_table']
).toDF()
```

**Reading from different layers:**
```python
# Read from bronze
bronze_df = glueContext.create_dynamic_frame.from_catalog(
    database=args['bronze_database'],
    table_name="fda_nsde"
).toDF()

# Read from silver
silver_df = glueContext.create_dynamic_frame.from_catalog(
    database=args['silver_database'],
    table_name="rxnorm_ndc_mappings"
).toDF()
```

---

## Delta Lake Configuration for Gold Layer (AWS Glue 5.0)

**CRITICAL: Proper Delta Lake configuration requires specific setup in both CDK and Python.**

**üìã See `DELTA_TABLE_REGISTRATION.md` for full architectural decision record.**

### CDK Stack Configuration (GoldStack.js)

#### 1. Glue Job Configuration

```javascript
defaultArguments: {
  // Enable Delta Lake support - loads libraries and modules
  '--datalake-formats': 'delta',

  // Deploy temporal versioning library
  '--extra-py-files': `${temporalLibPath}temporal_versioning_delta.py`,

  // DO NOT use --conf here! It causes "Invalid input to --conf" errors in Glue
  // Spark configs must be set in Python code BEFORE SparkContext creation
}
```

#### 2. Delta Table Registration (Pure IaC Pattern)

**üö´ DO NOT USE GLUE CRAWLERS for Delta Lake tables**

Crawlers incorrectly register Delta tables as `classification=parquet` instead of native Delta tables.

**‚úÖ USE `glue.CfnTable` - Pure CDK declarative resource (Production Pattern):**

```javascript
const cdk = require("aws-cdk-lib");
const glue = require("aws-cdk-lib/aws-glue");

// Gold table name (hyphens ‚Üí underscores for Glue)
const goldTableName = dataset.replace(/-/g, '_');
const goldBasePath = `s3://${bucketName}/gold/${dataset}/`;

// Defensive validation for S3 path
if (!goldBasePath.startsWith('s3://')) {
  throw new Error(`goldBasePath must be an S3 URI, got: ${goldBasePath}`);
}

// Register Delta Lake table using first-class CloudFormation resource
const goldTable = new glue.CfnTable(this, 'GoldTable', {
  databaseName: goldDatabase,
  catalogId: this.account,  // Resolves at deploy-time for multi-account compatibility
  tableInput: {
    name: goldTableName,
    description: `Gold layer Delta table for ${dataset} with SCD Type 2 temporal versioning`,
    tableType: 'EXTERNAL_TABLE',

    // CRITICAL: These parameters enable Athena Engine v3 native Delta reads
    parameters: {
      'classification': 'delta',
      'table_type': 'DELTA',
      'EXTERNAL': 'TRUE',
      'external': 'TRUE'  // Duplicate for AWS normalization across regions
    },

    // Storage descriptor with correct Parquet I/O formats for Delta
    // Schema Evolution: spark.databricks.delta.schema.autoMerge.enabled = true
    storageDescriptor: {
      location: goldBasePath,
      inputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
      outputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
      serdeInfo: {
        serializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
      }
      // columns array omitted - Delta manages schema via transaction log
    }
  }
});

// Explicit dependencies to prevent race conditions
goldTable.node.addDependency(goldJob);
goldTable.node.addDependency(dataWarehouseBucket);
```

**Production Hardening:**
- ‚úÖ `glue.CfnTable` - Pure CloudFormation (NOT Lambda-backed)
- ‚úÖ `this.account` - Deploy-time resolution (multi-account safe)
- ‚úÖ Dual `EXTERNAL`/`external` - AWS region normalization
- ‚úÖ S3 URI validation - Catches config errors at synth time
- ‚úÖ Explicit dependencies - Prevents race conditions
- ‚úÖ No columns array - Delta manages schema automatically
- ‚úÖ Fully declarative - No procedural logic, no IAM complexity

**See `DELTA_TABLE_REGISTRATION.md` for complete architectural decision record.**

### Python Job Configuration (gold_job.py)

**‚úÖ CORRECT: Configure Spark BEFORE creating SparkContext**

```python
from pyspark import SparkConf
from pyspark.context import SparkContext
from awsglue.context import GlueContext

# Set Delta Lake configs BEFORE SparkContext creation
conf = SparkConf()
conf.set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
conf.set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

# Initialize with configured SparkConf
sc = SparkContext(conf=conf)
glueContext = GlueContext(sc)
spark = glueContext.spark_session
```

**‚ùå WRONG: Do NOT set configs after SparkContext creation**

```python
sc = SparkContext()  # SparkSession created here
spark.conf.set(...)  # TOO LATE! Causes "Cannot modify static config" error
```

### Common Delta Lake Errors

| Error | Cause | Solution |
|-------|-------|----------|
| `ModuleNotFoundError: No module named 'delta'` | Missing `--datalake-formats` | Add `'--datalake-formats': 'delta'` in CDK |
| `Cannot modify the value of a static config` | Setting config after SparkContext | Use `SparkConf` before `SparkContext()` |
| `Invalid input to --conf` | Using `--conf` in CDK arguments | Remove `--conf`, use Python `SparkConf` instead |
| `Delta operation requires SparkSession to be configured` | Missing Spark extensions/catalog | Set via `SparkConf` in Python before context creation |
| Athena returns 0 rows despite data in S3 | Crawler registered table as `classification=parquet` | Use AwsCustomResource pattern (see above) |
| `TABLE_OR_VIEW_NOT_FOUND` | Table not registered in Glue catalog | Deploy CDK stack with AwsCustomResource |

### Stack Dependencies

Always ensure scripts are deployed before job creation:

```javascript
const scriptDeployment = new s3deploy.BucketDeployment(this, 'DeployGlueScript', {...});
const temporalDeployment = new s3deploy.BucketDeployment(this, 'DeployTemporalLib', {...});

// Prevent race conditions
goldJob.node.addDependency(scriptDeployment);
goldJob.node.addDependency(temporalDeployment);
```

---

## Data Flow

**Bronze Layer** (automated via factory + shared `bronze_http_job.py`):
1. Download ZIP from `source_url`
2. Extract files to `s3://.../raw/{dataset}/run_id={timestamp}/` (lineage tracking)
3. Read CSV files from raw layer
4. Apply `column_schema` transformations (rename, type cast, dates, etc.)
5. Add `meta_run_id` metadata column
6. Write to `s3://.../bronze/{dataset}/[{table}/]` as Parquet/ZSTD (kill-and-fill)
7. Crawler updates Glue catalog

**Key Features**:
- Runtime utilities (`etl_runtime_utils.py`) packaged via `--extra-py-files`
- Schema transformations support: `string`, `date`, `integer`, `long`, `float`, `double`, `decimal`, `boolean`
- Single-table datasets write to `bronze/{dataset}/`, multi-table to `bronze/{dataset}/{table}/`
- Raw layer preserves complete lineage with run_id partitions

**Silver Layer** (custom transformations):
- Read multiple bronze tables from Glue catalog
- Join/transform/aggregate as needed
- Write to `s3://.../silver/{dataset}/` as Parquet/ZSTD
- Crawler updates Glue catalog

**Gold Layer** (temporal versioning):
- Read silver tables from Glue catalog
- Apply temporal versioning via shared library
- Compare incoming data with existing gold table (MD5 content hash)
- Write new/changed/expired versions to `s3://.../gold/{dataset}/` as Parquet/ZSTD
- Partition by `status` (current/historical)
- Crawler updates Glue catalog

## Adding New Datasets

**Bronze - Pattern A (Factory-Based)**:
1. Create `datasets/bronze/{dataset}/config.json` with schema
2. Create `datasets/bronze/{dataset}/{Dataset}Stack.js` using factory pattern
3. Add to `index.js` under "// Import dataset stacks - Bronze"

**Bronze - Pattern B (Custom)**:
1. Create `datasets/bronze/{dataset}/config.json` (optional, for metadata)
2. Create custom `datasets/bronze/{dataset}/{Dataset}Stack.js` with manual Glue job setup
3. Create custom `datasets/bronze/{dataset}/glue/bronze_job.py` with specialized logic
4. Add to `index.js` under "// Import dataset stacks - Bronze"

**Silver** (always custom):
1. Create `datasets/silver/{dataset}/config.json` with source dependencies
2. Create custom `datasets/silver/{dataset}/{Dataset}Stack.js` (read source configs, pass table names)
3. Create `datasets/silver/{dataset}/glue/silver_job.py` with transformation logic
4. Add to `index.js` under "// Import dataset stacks - Silver"

**Gold** (always custom, uses temporal versioning):
1. Create `datasets/gold/{dataset}/config.json` with:
   - Source dependencies (silver or bronze tables)
   - `temporal_config`: `business_key` (string or array for composite keys), `business_columns` for change detection
   - **Use plural dataset names** (e.g., `fda-all-ndcs`, `rxnorm-ndc-mappings`)
2. Create custom `datasets/gold/{dataset}/{Dataset}Stack.js`
   - Deploy temporal library: `shared/runtime/temporal/temporal_versioning.py`
   - Pass library path via `--extra-py-files`
3. Create `datasets/gold/{dataset}/glue/gold_job.py`:
   - Import and use `apply_temporal_versioning()` from shared library
   - Drop layer-specific prefixes from column names
   - Support composite keys (pass array to `business_key` parameter)
4. Add to `index.js` under "// Import dataset stacks - Gold"

**RAWS Conventions**:
- All datasets MUST be organized into bronze/, silver/, or gold/ subdirectories based on their medallion layer
- Gold datasets use **plural names** for consistency
- Composite keys are supported for N:M relationships (e.g., rxcui + ndc_11, rxcui + class_id + class_type)
